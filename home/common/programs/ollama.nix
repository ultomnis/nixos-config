{
  # Run large language models locally
  services.ollama.enable = true;
}
